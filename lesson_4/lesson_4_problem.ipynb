{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-LAB LESSON 4: Markov Decision Process\n",
    "\n",
    "In the third session we will work on the Markov decision process (MDP)\n",
    "\n",
    "## Lava environments\n",
    "The environments used are LavaFloor (visible in the figure) and its variations.\n",
    "\n",
    "![Lava](images/lava.png)\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls of the previous environments, the floor is covered with lava, there is a black pit of death.\n",
    "\n",
    "Moreover, the agent can't comfortably perform its actions that instead have a stochastic outcome (visible in the figure):\n",
    "\n",
    "![Dynact](images/dynact.png)\n",
    "\n",
    "The action dynamics is the following:\n",
    "- $P(0.8)$ of moving in the desired direction\n",
    "- $P(0.1)$ of moving in a direction 90Â° with respect to the desired direction\n",
    "\n",
    "Finally, since the floor is covered in lava, the agent receives a negative reward for each of its steps!\n",
    "\n",
    "- -0.04 for each lava cell (L)\n",
    "- -5 for the black pit (P). End of episode\n",
    "- +1 for the treasure (G). End of episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "module_path = os.path.abspath(os.path.join('../tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Properties \n",
    "\n",
    "In addition to the varables of the environments you have been using in the previous sessions, there are also a few more:\n",
    "\n",
    "- $T$: matrix of the transition function $T(s, a, s') \\rightarrow [0, 1]$\n",
    "- $RS$: matrix of the reward function $R(s) \\rightarrow \\mathbb{R}$\n",
    "\n",
    "The available actions are still Left, Right, Up, Down.\n",
    "\n",
    "#### Code Hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LavaFloor-v0\")\n",
    "\n",
    "current_state = env.pos_to_state(0, 0)\n",
    "next_state = env.pos_to_state(0, 1)\n",
    "goal_state = env.pos_to_state(2, 3)\n",
    "\n",
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Actions: \", env.actions)\n",
    "print(\"Reward of starting state:\", env.RS[current_state])\n",
    "print(\"Reward of goal state:\", env.RS[goal_state])\n",
    "print(\"Probability from (0, 0) to (1, 0) with action left:\", env.T[current_state, 1, next_state])\n",
    "print(\"Probability from (0, 0) to (2, 3) with action left:\", env.T[current_state, 1, goal_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LavaFloor-v0\")\n",
    "\n",
    "current_state = env.pos_to_state(0, 0)\n",
    "stato = env.sample(current_state, 0)\n",
    "print(stato)\n",
    "\n",
    "print(env.T[current_state, 0, stato])\n",
    "print(env.T[current_state, 1, stato])\n",
    "print(env.T[current_state, 2, stato])\n",
    "print(env.T[current_state, 3, stato])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: Value Iteration Algorithm\n",
    "\n",
    "Your first assignment is to implement the Value Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state.  You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*, *NiceLavaFloor* and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/value-iteration.png\" width=\"600\">\n",
    "\n",
    "The *value_iteration* function has to be implemented. Notice that the value iteration approach return a matrix with the value for eacht state, the function *values_to_policy* automatically convert this matrix in the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, maxiters=300, discount=0.9, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        max_error: the maximum error allowd in the utility of any state\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)] # vector of utilities for states S\n",
    "    delta = 0 # maximum change in the utility o any state in an iteration\n",
    "    U = U_1.copy()\n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    return values_to_policy(np.asarray(U), env) # automatically convert the value matrix U to a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code executes and Value Iteration and prints the resulting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envname = \"LavaFloor-v0\"\n",
    "#envname = \"HugeLavaFloor-v0\"\n",
    "#envname = \"NiceLavaFloor-v0\"\n",
    "#envname = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "print(\"\\tEnvironment: {} \\n\\tValue Iteration\".format(envname))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "env = gym.make(envname)\n",
    "print(\"\\nRENDER:\")\n",
    "env.render()\n",
    "\n",
    "t = timer()\n",
    "policy = value_iteration(env)\n",
    "\n",
    "print(\"\\nTIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "print(\"\\nPOLICY:\")\n",
    "print(np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results can be found [here](lesson_4_results.txt).\n",
    "\n",
    "### Assignment 2: Policy Iteration Algorithm\n",
    "\n",
    "Your first assignment is to implement the Policy Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*, *NiceLavaFloor* and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/policy-iteration.png\" width=\"600\">\n",
    "\n",
    "For the *policy evaluation step*, it is necessary to implement this function:\n",
    "\n",
    "<img src=\"images/policy-evaluating.png\" width=\"500\">\n",
    "\n",
    "The following function has to be implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, maxiters=150, discount=0.9, maxviter=10):\n",
    "    \"\"\"\n",
    "    Performs the policy iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    policy = [0 for _ in range(environment.observation_space.n)] #initial policy\n",
    "    U = [0 for _ in range(environment.observation_space.n)] #utility array\n",
    "\n",
    "    # Step (1): Policy Evaluation\n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    \n",
    "    # Step (2) Policy Improvement\n",
    "    unchanged = True  \n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    \n",
    "    return np.asarray(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code executes and Value Iteration and prints the resulting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envname = \"LavaFloor-v0\"\n",
    "#envname = \"HugeLavaFloor-v0\"\n",
    "#envname = \"NiceLavaFloor-v0\"\n",
    "#envname = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "print(\"\\tEnvironment: {} \\n\\tPolicy Iteration\".format(envname))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "env = gym.make(envname)\n",
    "print(\"\\nRENDER:\")\n",
    "env.render()\n",
    "\n",
    "t = timer()\n",
    "policy = policy_iteration(env)\n",
    "\n",
    "print(\"\\nTIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "print(\"\\nPOLICY:\")\n",
    "print(np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results can be found [here](lesson_4_results.txt).\n",
    "\n",
    "### Comparison\n",
    "\n",
    "The following code performs a comparison between Value Iteration and Policy Iteration, by plotting the accumulated rewards of each episode with iterations in range $[1, 50]$ (might take a long time if not optimizied via numpy). You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*.\n",
    "\n",
    "The function **run_episode(envirnonment, policy, max_iteration)** run an episode on the given environment using the input policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envname = \"LavaFloor-v0\"\n",
    "#envname = \"HugeLavaFloor-v0\"\n",
    "\n",
    "maxiters = 49\n",
    "\n",
    "env = gym.make(envname)\n",
    "\n",
    "series = []  # Series of learning rates to plot\n",
    "liters = np.arange(maxiters + 1)  # Learning iteration values\n",
    "liters[0] = 1\n",
    "elimit = 100  # Limit of steps per episode\n",
    "rep = 10  # Number of repetitions per iteration value\n",
    "virewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "gamma = 0.9\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Value iteration\n",
    "for i in tqdm(liters, desc=\"Value Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = value_iteration(env, maxiters=i)  # Compute policy\n",
    "        \n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    virewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": virewards, \"ls\": \"-\", \"label\": \"Value Iteration\"})\n",
    "\n",
    "\n",
    "vmaxiters = 10  # Max number of iterations to perform while evaluating a policy\n",
    "pirewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "# Policy iteration\n",
    "for i in tqdm(liters, desc=\"Policy Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = policy_iteration(env, i, gamma, vmaxiters)  # Compute policy\n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    pirewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": pirewards, \"ls\": \"-\", \"label\": \"Policy Iteration\"})\n",
    "\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "np.set_printoptions(linewidth=10000)\n",
    "\n",
    "plot(series, \"Learning Rate\", \"Iterations\", \"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for comparison can be found here below. Notice that since the executions are stochastic the charts could differ: the important thing is the global trend and the final convergence to an optimal solution.\n",
    "\n",
    "**Standard Lava floor results comparison**\n",
    "<img src=\"images/results-standard.png\" width=\"600\">\n",
    "\n",
    "**Huge Lava floor results comparison** \n",
    "<img src=\"images/results-huge.png\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
